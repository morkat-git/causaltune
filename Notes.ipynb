{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistc ERUPT in CATE Setting\n",
    "\n",
    "Let's suppose we have two treatments for a medical condition, and we want to evaluate their effectiveness for patients segmented by age group. Our goal is to use probabilistic ERUPT to compare different models that estimate CATE.\n",
    "\n",
    "#### Treatments\n",
    "\n",
    "- **Treatment A**: Medication A\n",
    "- **Treatment B**: Medication B\n",
    "\n",
    "#### Data from Clinical Trials\n",
    "\n",
    "Here is hypothetical data showing patient outcomes after receiving each treatment, segmented by age group:\n",
    "\n",
    "| Patient | Age Group | Treatment | Outcome |\n",
    "|---------|-----------|-----------|---------|\n",
    "| 1       | Young     | A         | 5       |\n",
    "| 2       | Young     | A         | 3       |\n",
    "| 3       | Young     | B         | 2       |\n",
    "| 4       | Old       | B         | 4       |\n",
    "| 5       | Old       | A         | 6       |\n",
    "| 6       | Old       | B         | 5       |\n",
    "\n",
    "#### Model Predictions\n",
    "\n",
    "Two different models provide CATE estimates (impact) and uncertainties (standard deviations) for each treatment, segmented by age group.\n",
    "\n",
    "**Model 1 Estimates:**\n",
    "\n",
    "| Age Group | Treatment | Estimated Impact | Std Deviation |\n",
    "|-----------|-----------|------------------|---------------|\n",
    "| Young     | A         | 4.5              | 1.0           |\n",
    "| Young     | B         | 3.5              | 0.5           |\n",
    "| Old       | A         | 5.5              | 0.8           |\n",
    "| Old       | B         | 4.0              | 0.6           |\n",
    "\n",
    "**Model 2 Estimates:**\n",
    "\n",
    "| Age Group | Treatment | Estimated Impact | Std Deviation |\n",
    "|-----------|-----------|------------------|---------------|\n",
    "| Young     | A         | 4.0              | 0.8           |\n",
    "| Young     | B         | 4.0              | 0.8           |\n",
    "| Old       | A         | 5.0              | 0.7           |\n",
    "| Old       | B         | 5.0              | 0.7           |\n",
    "\n",
    "### Probabilistic ERUPT Process in CATE Setting\n",
    "\n",
    "Here’s how the process can be adapted to evaluate and compare these models in the CATE setting:\n",
    "\n",
    "1. **Probabilistic Treatment Selection Based on CATE**:\n",
    "   - We use Thompson sampling to select treatments probabilistically based on the CATE estimates and their uncertainties.\n",
    "\n",
    "   For each age group, we sample from the distributions defined by the models' estimates.\n",
    "\n",
    "   **Example of one iteration for each model:**\n",
    "\n",
    "   - **Model 1**:\n",
    "     - **Young Group**: Samples 4.2 for A and 3.8 for B — chooses A.\n",
    "     - **Old Group**: Samples 5.1 for A and 3.7 for B — chooses A.\n",
    "\n",
    "   - **Model 2**:\n",
    "     - **Young Group**: Samples 3.5 for A and 4.5 for B — chooses B.\n",
    "     - **Old Group**: Samples 4.6 for A and 5.2 for B — chooses B.\n",
    "\n",
    "2. **Simulate the Selection Process**:\n",
    "   - Perform this sampling many times to simulate different scenarios.\n",
    "   - Calculate how often each treatment is chosen in each age group.\n",
    "\n",
    "3. **Calculate ERUPT Scores**:\n",
    "   - For each model, after many iterations, calculate the average outcome for the treatments chosen based on the probabilistic CATE estimates.\n",
    "\n",
    "   Historical outcomes based on the table:\n",
    "\n",
    "   - **Young Group**: Average outcomes for A: 4, for B: 2.\n",
    "   - **Old Group**: Average outcomes for A: 6, for B: 4.5.\n",
    "\n",
    "   **Calculated over many iterations:**\n",
    "\n",
    "   - **Model 1**: \n",
    "     - Young Group: More often chooses A.\n",
    "     - Old Group: More often chooses A.\n",
    "     - Average outcome = $(0.7 * 4 + 0.3 * 2)_{Young} + (0.8 * 6 + 0.2 * 4.5)_{Old}$\n",
    "     - $= (0.7 * 4 + 0.3 * 2) + (0.8 * 6 + 0.2 * 4.5)$\n",
    "     - $= (2.8 + 0.6) + (4.8 + 0.9) = 3.4 + 5.7 = 9.$\n",
    "\n",
    "   - **Model 2**: \n",
    "     - Young Group: More often chooses B.\n",
    "     - Old Group: More often chooses B.\n",
    "     - Average outcome = $(0.4 * 4 + 0.6 * 2)_{Young} + (0.4 * 6 + 0.6 * 4.5)_{Old}$\n",
    "     - $= (0.4 * 4 + 0.6 * 2) + (0.4 * 6 + 0.6 * 4.5)$\n",
    "     - $= (1.6 + 1.2) + (2.4 + 2.7) = 2.8 + 5.1 = 7.9$\n",
    "\n",
    "4. **Compare Models**:\n",
    "   - Compare these average outcomes. The model with the higher average outcome is considered better at using CATE for treatment selection.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this example, **Model 1** appears to perform better in utilizing CATE to probabilistically select more effective treatments, based on the outcomes in each segment. This approach shows how probabilistic ERUPT can be a powerful tool for comparing models in terms of their ability to use CATE estimates effectively under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suitable Estimators (Provide unertainty naturally)\n",
    "\n",
    "Naturally:\n",
    "- Causal Forest DML: Provides uncertainties naturally from the forest structure.\n",
    "- DR Ortho Forest: Provides standard errors naturally.\n",
    "- DML Ortho Forest: Also provides standard errors naturally.\n",
    "\n",
    "\n",
    "With conditions:\n",
    "- T-Learner: If the underlying models provide standard deviations.\n",
    "- X-Learner: If the second-stage models provide standard deviations.\n",
    "- Forest DR Learner: Can provide standard deviations through the variability of forest predictions.\n",
    "- Linear DR Learner: Can provide standard errors for predictions.\n",
    "- Sparse Linear DR Learner: Can provide uncertainties through methods like bootstrapping.\n",
    "- Linear DML: Provides standard errors if the outcome model is linear.\n",
    "- Sparse Linear DML: Similar to Linear DML with potential bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import math\n",
    "from typing import Optional, Dict, Union, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "from econml.cate_interpreter import SingleTreeCateInterpreter  # noqa F401\n",
    "from dowhy.causal_estimator import CausalEstimate\n",
    "from dowhy import CausalModel\n",
    "\n",
    "\n",
    "from causaltune.thirdparty.causalml import metrics\n",
    "from causaltune.erupt import ERUPT\n",
    "from causaltune.utils import treatment_values, psw_joint_weights\n",
    "\n",
    "import dcor\n",
    "\n",
    "\n",
    "class DummyEstimator:\n",
    "    def __init__(\n",
    "        self, cate_estimate: np.ndarray, effect_intervals: Optional[np.ndarray] = None\n",
    "    ):\n",
    "        self.cate_estimate = cate_estimate\n",
    "        self.effect_intervals = effect_intervals\n",
    "\n",
    "    def const_marginal_effect(self, X):\n",
    "        return self.cate_estimate\n",
    "\n",
    "\n",
    "def supported_metrics(problem: str, multivalue: bool, scores_only: bool) -> List[str]:\n",
    "    if problem == \"iv\":\n",
    "        metrics = [\"energy_distance\"]\n",
    "        if not scores_only:\n",
    "            metrics.append(\"ate\")\n",
    "        return metrics\n",
    "    elif problem == \"backdoor\":\n",
    "        print(\"backdoor\")\n",
    "        if multivalue:\n",
    "            # TODO: support other metrics for the multivalue case\n",
    "            return [\"energy_distance\", \"psw_energy_distance\"]\n",
    "        else:\n",
    "            metrics = [\n",
    "                \"erupt\",\n",
    "                \"norm_erupt\",\n",
    "                \"prob_erupt\",\n",
    "                \"qini\",\n",
    "                \"auc\",\n",
    "                # \"r_scorer\",\n",
    "                \"energy_distance\",\n",
    "                \"psw_energy_distance\",\n",
    "            ]\n",
    "            if not scores_only:\n",
    "                metrics.append(\"ate\")\n",
    "            return metrics\n",
    "\n",
    "\n",
    "class Scorer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        causal_model: CausalModel,\n",
    "        propensity_model: Any,\n",
    "        problem: str,\n",
    "        multivalue: bool,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Contains scoring logic for CausalTune.\n",
    "\n",
    "        Access methods and attributes via `CausalTune.scorer`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.problem = problem\n",
    "        self.multivalue = multivalue\n",
    "        self.causal_model = copy.deepcopy(causal_model)\n",
    "\n",
    "        self.identified_estimand = causal_model.identify_effect(\n",
    "            proceed_when_unidentifiable=True\n",
    "        )\n",
    "\n",
    "        if problem == \"backdoor\":\n",
    "            print(\n",
    "                \"Fitting a Propensity-Weighted scoring estimator to be used in scoring tasks\"\n",
    "            )\n",
    "            treatment_series = causal_model._data[causal_model._treatment[0]]\n",
    "            # this will also fit self.propensity_model, which we'll also use in self.erupt\n",
    "            self.psw_estimator = self.causal_model.estimate_effect(\n",
    "                self.identified_estimand,\n",
    "                method_name=\"backdoor.causaltune.models.MultivaluePSW\",\n",
    "                control_value=0,\n",
    "                treatment_value=treatment_values(treatment_series, 0),\n",
    "                target_units=\"ate\",  # condition used for CATE\n",
    "                confidence_intervals=False,\n",
    "                method_params={\n",
    "                    \"init_params\": {\"propensity_model\": propensity_model},\n",
    "                },\n",
    "            ).estimator\n",
    "\n",
    "            treatment_name = self.psw_estimator._treatment_name\n",
    "            if not isinstance(treatment_name, str):\n",
    "                treatment_name = treatment_name[0]\n",
    "\n",
    "            # No need to call self.erupt.fit() as propensity model is already fitted\n",
    "            # self.propensity_model = est.propensity_model\n",
    "            self.erupt = ERUPT(\n",
    "                treatment_name=treatment_name,\n",
    "                propensity_model=self.psw_estimator.estimator.propensity_model,\n",
    "                X_names=self.psw_estimator._effect_modifier_names\n",
    "                + self.psw_estimator._observed_common_causes_names,\n",
    "            )\n",
    "\n",
    "    def ate(self, df: pd.DataFrame) -> tuple:\n",
    "        \"\"\"Calculate the Average Treatment Effect. Provide naive std estimates in single-treatment cases.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): input dataframe\n",
    "\n",
    "        Returns:\n",
    "            tuple: tuple containing the ATE, standard deviation of the estimate (or None if multi-treatment),\n",
    "                and sample size (or None if estimate has more than one dimension)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        estimate = self.psw_estimator.estimator.effect(df).mean(axis=0)\n",
    "\n",
    "        if len(estimate) == 1:\n",
    "            # for now, let's cheat on the std estimation, take that from the naive ate\n",
    "            treatment_name = self.causal_model._treatment[0]\n",
    "            outcome_name = self.causal_model._outcome[0]\n",
    "            naive_est = Scorer.naive_ate(df[treatment_name], df[outcome_name])\n",
    "            return estimate[0], naive_est[1], naive_est[2]\n",
    "        else:\n",
    "            return estimate, None, None\n",
    "\n",
    "    def resolve_metric(self, metric: str) -> str:\n",
    "        \"\"\"Check if supplied metric is supported. If not, default to 'energy_distance'.\n",
    "\n",
    "        Args:\n",
    "            metric (str): evaluation metric\n",
    "\n",
    "        Returns:\n",
    "            str: metric/'energy_distance'\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        metrics = supported_metrics(self.problem, self.multivalue, scores_only=True)\n",
    "\n",
    "        if metric not in metrics:\n",
    "            logging.warning(\n",
    "                f\"Using energy_distance metric as {metric} is not in the list \"\n",
    "                f\"of supported metrics for this usecase ({str(metrics)})\"\n",
    "            )\n",
    "            return \"energy_distance\"\n",
    "        else:\n",
    "            return metric\n",
    "\n",
    "    def resolve_reported_metrics(\n",
    "        self, metrics_to_report: Union[List[str], None], scoring_metric: str\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Check if supplied reporting metrics are valid.\n",
    "\n",
    "        Args:\n",
    "            metrics_to_report (Union[List[str], None]): list of strings specifying the evaluation metrics to compute.\n",
    "                Possible options include 'ate', 'erupt', 'norm_erupt', 'qini', 'auc',\n",
    "                'energy_distance' and 'psw_energy_distance'.\n",
    "            scoring_metric (str): specified metric\n",
    "\n",
    "        Returns:\n",
    "            List[str]: list of valid metrics\n",
    "        \"\"\"\n",
    "\n",
    "        metrics = supported_metrics(self.problem, self.multivalue, scores_only=False)\n",
    "        if metrics_to_report is None:\n",
    "            return metrics\n",
    "        else:\n",
    "            metrics_to_report = sorted(list(set(metrics_to_report + [scoring_metric])))\n",
    "            for m in metrics_to_report.copy():\n",
    "                if m not in metrics:\n",
    "                    logging.warning(\n",
    "                        f\"Dropping the metric {m} for problem: {self.problem} \\\n",
    "                        : must be one of {metrics}\"\n",
    "                    )\n",
    "                    metrics_to_report.remove(m)\n",
    "        return metrics_to_report\n",
    "\n",
    "    @staticmethod\n",
    "    def energy_distance_score(\n",
    "        estimate: CausalEstimate,\n",
    "        df: pd.DataFrame,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate energy distance score between treated and controls.\n",
    "        For theoretical details, see Ramos-Carreño and Torrecilla (2023).\n",
    "\n",
    "        Args:\n",
    "            estimate (dowhy.causal_estimator.CausalEstimate): causal estimate to evaluate\n",
    "            df (pandas.DataFrame): input dataframe\n",
    "\n",
    "        Returns:\n",
    "            float: energy distance score\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Y0X, _, split_test_by = Scorer._Y0_X_potential_outcomes(estimate, df)\n",
    "\n",
    "        YX_1 = Y0X[Y0X[split_test_by] == 1]\n",
    "        YX_0 = Y0X[Y0X[split_test_by] == 0]\n",
    "        select_cols = estimate.estimator._effect_modifier_names + [\"yhat\"]\n",
    "\n",
    "        energy_distance_score = dcor.energy_distance(\n",
    "            YX_1[select_cols], YX_0[select_cols]\n",
    "        )\n",
    "\n",
    "        return energy_distance_score\n",
    "\n",
    "    @staticmethod\n",
    "    def _Y0_X_potential_outcomes(estimate: CausalEstimate, df: pd.DataFrame):\n",
    "        est = estimate.estimator\n",
    "        # assert est.identifier_method in [\"iv\", \"backdoor\"]\n",
    "        treatment_name = (\n",
    "            est._treatment_name\n",
    "            if isinstance(est._treatment_name, str)\n",
    "            else est._treatment_name[0]\n",
    "        )\n",
    "        df[\"dy\"] = estimate.estimator.effect_tt(df)\n",
    "        df[\"yhat\"] = df[est._outcome_name] - df[\"dy\"]\n",
    "\n",
    "        split_test_by = (\n",
    "            est.estimating_instrument_names[0]\n",
    "            if est.identifier_method == \"iv\"\n",
    "            else treatment_name\n",
    "        )\n",
    "\n",
    "        Y0X = copy.deepcopy(df)\n",
    "        return Y0X, treatment_name, split_test_by\n",
    "\n",
    "    def psw_energy_distance(\n",
    "        self,\n",
    "        estimate: CausalEstimate,\n",
    "        df: pd.DataFrame,\n",
    "        normalise_features=False,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate propensity score adjusted energy distance score between treated and controls.\n",
    "\n",
    "        Features are normalised using the sklearn.preprocessing.QuantileTransformer\n",
    "\n",
    "        For theoretical details, see Ramos-Carreño and Torrecilla (2023).\n",
    "\n",
    "        @param estimate (dowhy.causal_estimator.CausalEstimate): causal estimate to evaluate\n",
    "        @param df (pandas.DataFrame): input dataframe\n",
    "        @param normalise_features (bool): whether to normalise features with QuantileTransformer\n",
    "\n",
    "        @return float: propensity-score weighted energy distance score\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Y0X, treatment_name, split_test_by = Scorer._Y0_X_potential_outcomes(\n",
    "            estimate, df\n",
    "        )\n",
    "\n",
    "        Y0X_1 = Y0X[Y0X[split_test_by] == 1]\n",
    "        Y0X_0 = Y0X[Y0X[split_test_by] == 0]\n",
    "\n",
    "        YX_1_all_psw = self.psw_estimator.estimator.propensity_model.predict_proba(\n",
    "            Y0X_1[\n",
    "                self.causal_model.get_effect_modifiers()\n",
    "                + self.causal_model.get_common_causes()\n",
    "            ]\n",
    "        )\n",
    "        treatment_series = Y0X_1[treatment_name]\n",
    "\n",
    "        YX_1_psw = np.zeros(YX_1_all_psw.shape[0])\n",
    "        for i in treatment_series.unique():\n",
    "            YX_1_psw[treatment_series == i] = YX_1_all_psw[:, i][treatment_series == i]\n",
    "\n",
    "        YX_0_psw = self.psw_estimator.estimator.propensity_model.predict_proba(\n",
    "            Y0X_0[\n",
    "                self.causal_model.get_effect_modifiers()\n",
    "                + self.causal_model.get_common_causes()\n",
    "            ]\n",
    "        )[:, 0]\n",
    "\n",
    "        select_cols = estimate.estimator._effect_modifier_names + [\"yhat\"]\n",
    "        features = estimate.estimator._effect_modifier_names\n",
    "\n",
    "        xy_psw = psw_joint_weights(YX_1_psw, YX_0_psw)\n",
    "        xx_psw = psw_joint_weights(YX_0_psw)\n",
    "        yy_psw = psw_joint_weights(YX_1_psw)\n",
    "\n",
    "        xy_mean_weights = np.mean(xy_psw)\n",
    "        xx_mean_weights = np.mean(xx_psw)\n",
    "        yy_mean_weights = np.mean(yy_psw)\n",
    "\n",
    "        if normalise_features:\n",
    "            qt = QuantileTransformer(n_quantiles=200)\n",
    "            X_quantiles = qt.fit_transform(Y0X[features])\n",
    "\n",
    "            Y0X_transformed = pd.DataFrame(\n",
    "                X_quantiles, columns=features, index=Y0X.index\n",
    "            )\n",
    "            Y0X_transformed.loc[:, [\"yhat\", split_test_by]] = Y0X[\n",
    "                [\"yhat\", split_test_by]\n",
    "            ]\n",
    "\n",
    "            Y0X_1 = Y0X_transformed[Y0X_transformed[split_test_by] == 1]\n",
    "            Y0X_0 = Y0X_transformed[Y0X_transformed[split_test_by] == 0]\n",
    "\n",
    "        exponent = 1\n",
    "        distance_xy = np.reciprocal(xy_mean_weights) * np.multiply(\n",
    "            xy_psw,\n",
    "            dcor.distances.pairwise_distances(\n",
    "                Y0X_1[select_cols], Y0X_0[select_cols], exponent=exponent\n",
    "            ),\n",
    "        )\n",
    "        distance_yy = np.reciprocal(yy_mean_weights) * np.multiply(\n",
    "            yy_psw,\n",
    "            dcor.distances.pairwise_distances(Y0X_1[select_cols], exponent=exponent),\n",
    "        )\n",
    "        distance_xx = np.reciprocal(xx_mean_weights) * np.multiply(\n",
    "            xx_psw,\n",
    "            dcor.distances.pairwise_distances(Y0X_0[select_cols], exponent=exponent),\n",
    "        )\n",
    "        psw_energy_distance = (\n",
    "            2 * np.mean(distance_xy) - np.mean(distance_xx) - np.mean(distance_yy)\n",
    "        )\n",
    "        return psw_energy_distance\n",
    "\n",
    "    @staticmethod\n",
    "    def qini_make_score(\n",
    "        estimate: CausalEstimate, df: pd.DataFrame, cate_estimate: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the Qini score, defined as the area between the Qini curves of a model and random.\n",
    "\n",
    "        Args:\n",
    "            estimate (dowhy.causal_estimator.CausalEstimate): causal estimate to evaluate\n",
    "            df (pandas.DataFrame): input dataframe\n",
    "            cate_estimate (np.ndarray): array with cate estimates\n",
    "\n",
    "        Returns:\n",
    "            float: Qini score\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        est = estimate.estimator\n",
    "        new_df = pd.DataFrame()\n",
    "        new_df[\"y\"] = df[est._outcome_name]\n",
    "        treatment_name = est._treatment_name\n",
    "        if not isinstance(treatment_name, str):\n",
    "            treatment_name = treatment_name[0]\n",
    "        new_df[\"w\"] = df[treatment_name]\n",
    "        new_df[\"model\"] = cate_estimate\n",
    "\n",
    "        qini_score = metrics.qini_score(new_df)\n",
    "\n",
    "        return qini_score[\"model\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def auc_make_score(\n",
    "        estimate: CausalEstimate, df: pd.DataFrame, cate_estimate: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the area under the uplift curve.\n",
    "\n",
    "        Args:\n",
    "            estimate (dowhy.causal_estimator.CausalEstimate): causal estimate to evaluate\n",
    "            df (pandas.DataFrame): input dataframe\n",
    "            cate_estimate (np.ndarray): array with cate estimates\n",
    "\n",
    "        Returns:\n",
    "            float: area under the uplift curve\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        est = estimate.estimator\n",
    "        new_df = pd.DataFrame()\n",
    "        new_df[\"y\"] = df[est._outcome_name]\n",
    "        treatment_name = est._treatment_name\n",
    "        if not isinstance(treatment_name, str):\n",
    "            treatment_name = treatment_name[0]\n",
    "        new_df[\"w\"] = df[treatment_name]\n",
    "        new_df[\"model\"] = cate_estimate\n",
    "\n",
    "        auc_score = metrics.auuc_score(new_df)\n",
    "\n",
    "        return auc_score[\"model\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def real_qini_make_score(\n",
    "        estimate: CausalEstimate, df: pd.DataFrame, cate_estimate: np.ndarray\n",
    "    ) -> float:\n",
    "        # TODO  To calculate the 'real' qini score for synthetic datasets, to be done\n",
    "\n",
    "        # est = estimate.estimator\n",
    "        new_df = pd.DataFrame()\n",
    "\n",
    "        # new_df['tau'] = [df['y_factual'] - df['y_cfactual']]\n",
    "        new_df[\"model\"] = cate_estimate\n",
    "\n",
    "        qini_score = metrics.qini_score(new_df)\n",
    "\n",
    "        return qini_score[\"model\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def r_make_score(\n",
    "        estimate: CausalEstimate, df: pd.DataFrame, cate_estimate: np.ndarray, r_scorer\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate r_score.\n",
    "        For details refer to Nie and Wager (2017) and Schuler et al. (2018). Adaption from EconML implementation.\n",
    "\n",
    "        Args:\n",
    "            estimate (dowhy.causal_estimator.CausalEstimate): causal estimate to evaluate\n",
    "            df (pandas.DataFrame): input dataframe\n",
    "            cate_estimate (np.ndarray): array with cate estimates\n",
    "            r_scorer: callable object used to compute the R-score\n",
    "\n",
    "        Returns:\n",
    "            float: r_score\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "        return r_scorer.score(cate_estimate)\n",
    "\n",
    "    @staticmethod\n",
    "    def naive_ate(treatment: pd.Series, outcome: pd.Series):\n",
    "        \"\"\"Calculate simple ATE.\n",
    "\n",
    "        Args:\n",
    "            treatment (pandas.Series): series of treatments\n",
    "            outcome (pandas.Series): series of outcomes\n",
    "\n",
    "        Returns:\n",
    "            tuple: tuple of simple ATE, standard deviation, and sample size\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        treated = (treatment == 1).sum()\n",
    "\n",
    "        mean_ = outcome[treatment == 1].mean() - outcome[treatment == 0].mean()\n",
    "        std1 = outcome[treatment == 1].std() / (math.sqrt(treated) + 1e-3)\n",
    "        std2 = outcome[treatment == 0].std() / (\n",
    "            math.sqrt(len(outcome) - treated) + 1e-3\n",
    "        )\n",
    "        std_ = math.sqrt(std1 * std1 + std2 * std2)\n",
    "        return (mean_, std_, len(treatment))\n",
    "\n",
    "    def group_ate(\n",
    "        self, df: pd.DataFrame, policy: Union[pd.DataFrame, np.ndarray]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute the average treatment effect (ATE) for different groups specified by a policy.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): input dataframe, should contain columns for the treatment, outcome, and policy\n",
    "            policy (Union[pd.DataFrame, np.ndarray]): policy column in df or an array of the policy values,\n",
    "                used to group the data\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: ATE, std, and size per policy\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        tmp = {\"all\": self.ate(df)}\n",
    "        for p in sorted(list(policy.unique())):\n",
    "            tmp[p] = self.ate(df[policy == p])\n",
    "\n",
    "        tmp2 = [\n",
    "            {\"policy\": str(p), \"mean\": m, \"std\": s, \"count\": c}\n",
    "            for p, (m, s, c) in tmp.items()\n",
    "        ]\n",
    "\n",
    "        return pd.DataFrame(tmp2)\n",
    "\n",
    "    def make_scores(\n",
    "        self,\n",
    "        estimate: CausalEstimate,\n",
    "        df: pd.DataFrame,\n",
    "        metrics_to_report: List[str],\n",
    "        r_scorer=None,\n",
    "    ) -> dict:\n",
    "        \"\"\"Calculate various performance metrics for a given causal estimate using a given DataFrame.\n",
    "\n",
    "        Args:\n",
    "            estimate (dowhy.causal_estimator.CausalEstimate): causal estimate to evaluate\n",
    "            df (pandas.DataFrame): input dataframe\n",
    "            metrics_to_report (List[str]): list of strings specifying the evaluation metrics to compute.\n",
    "                Possible options include 'ate', 'erupt', 'norm_erupt', 'qini', 'auc',\n",
    "                'energy_distance' and 'psw_energy_distance'.\n",
    "            r_scorer (Optional): callable object used to compute the R-score, default is None\n",
    "\n",
    "        Returns:\n",
    "            dict: dictionary containing the evaluation metrics specified in metrics_to_report.\n",
    "                The values key in the dictionary contains the input DataFrame with additional columns for\n",
    "                the propensity scores, the policy, the normalized policy, and the weights, if applicable.\n",
    "        \"\"\"\n",
    "\n",
    "        out = dict()\n",
    "        df = df.copy().reset_index()\n",
    "\n",
    "        est = estimate.estimator\n",
    "        treatment_name = est._treatment_name\n",
    "        if not isinstance(treatment_name, str):\n",
    "            treatment_name = treatment_name[0]\n",
    "        outcome_name = est._outcome_name\n",
    "\n",
    "        cate_estimate = est.effect(df)\n",
    "\n",
    "        # TODO: fix this hack with proper treatment of multivalues\n",
    "        if len(cate_estimate.shape) > 1 and cate_estimate.shape[1] == 1:\n",
    "            cate_estimate = cate_estimate.reshape(-1)\n",
    "\n",
    "        # TODO: fix this, currently broken\n",
    "        # covariates = est._effect_modifier_names\n",
    "        # Include CATE Interpereter for both IV and CATE models\n",
    "        # intrp = SingleTreeCateInterpreter(\n",
    "        #     include_model_uncertainty=False, max_depth=2, min_samples_leaf=10\n",
    "        # )\n",
    "        # intrp.interpret(DummyEstimator(cate_estimate), df[covariates])\n",
    "        # intrp.feature_names = covariates\n",
    "        # out[\"intrp\"] = intrp\n",
    "\n",
    "        if self.problem == \"backdoor\":\n",
    "            values = df[[treatment_name, outcome_name]]\n",
    "            simple_ate = self.ate(df)[0]\n",
    "            if isinstance(simple_ate, float):\n",
    "                # simple_ate = simple_ate[0]\n",
    "                # .reset_index(drop=True)\n",
    "                values[\n",
    "                    \"p\"\n",
    "                ] = self.psw_estimator.estimator.propensity_model.predict_proba(\n",
    "                    df[\n",
    "                        self.causal_model.get_effect_modifiers()\n",
    "                        + self.causal_model.get_common_causes()\n",
    "                    ]\n",
    "                )[\n",
    "                    :, 1\n",
    "                ]\n",
    "                values[\"policy\"] = cate_estimate > 0\n",
    "                values[\"norm_policy\"] = cate_estimate > simple_ate\n",
    "                values[\"weights\"] = self.erupt.weights(df, lambda x: cate_estimate > 0)\n",
    "            else:\n",
    "                pass\n",
    "                # TODO: what do we do here if multiple treatments?\n",
    "\n",
    "            if \"erupt\" in metrics_to_report:\n",
    "                erupt_score = self.erupt.score(df, df[outcome_name], cate_estimate > 0)\n",
    "                out[\"erupt\"] = erupt_score\n",
    "\n",
    "            if \"norm_erupt\" in metrics_to_report:\n",
    "                norm_erupt_score = (\n",
    "                    self.erupt.score(df, df[outcome_name], cate_estimate > simple_ate)\n",
    "                    - simple_ate * values[\"norm_policy\"].mean()\n",
    "                )\n",
    "                out[\"norm_erupt\"] = norm_erupt_score\n",
    "\n",
    "            if \"prob_erupt\" in metrics_to_report:\n",
    "                treatment_effects = pd.Series(cate_estimate, index=df.index)\n",
    "                treatment_std_devs = pd.Series(cate_estimate.std(), index=df.index)\n",
    "                prob_erupt_score = self.erupt.probabilistic_erupt_score(df, df[outcome_name], treatment_effects, treatment_std_devs)\n",
    "                out[\"prob_erupt\"] = prob_erupt_score\n",
    "\n",
    "            if \"qini\" in metrics_to_report:\n",
    "                out[\"qini\"] = Scorer.qini_make_score(estimate, df, cate_estimate)\n",
    "\n",
    "            if \"auc\" in metrics_to_report:\n",
    "                out[\"auc\"] = Scorer.auc_make_score(estimate, df, cate_estimate)\n",
    "\n",
    "            if r_scorer is not None:\n",
    "                out[\"r_score\"] = Scorer.r_make_score(\n",
    "                    estimate, df, cate_estimate, r_scorer\n",
    "                )\n",
    "\n",
    "            # values = values.rename(columns={treatment_name: \"treated\"})\n",
    "            assert len(values) == len(df), \"Index weirdness when adding columns!\"\n",
    "            values = values.copy()\n",
    "            out[\"values\"] = values\n",
    "\n",
    "        if \"ate\" in metrics_to_report:\n",
    "            out[\"ate\"] = cate_estimate.mean()\n",
    "            out[\"ate_std\"] = cate_estimate.std()\n",
    "\n",
    "        if \"energy_distance\" in metrics_to_report:\n",
    "            out[\"energy_distance\"] = Scorer.energy_distance_score(estimate, df)\n",
    "\n",
    "        if \"psw_energy_distance\" in metrics_to_report:\n",
    "            out[\"psw_energy_distance\"] = self.psw_energy_distance(\n",
    "                estimate,\n",
    "                df,\n",
    "            )\n",
    "\n",
    "        del df\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def best_score_by_estimator(\n",
    "        scores: Dict[str, dict], metric: str\n",
    "    ) -> Dict[str, dict]:\n",
    "        \"\"\"Obtain best score for each estimator.\n",
    "\n",
    "        Args:\n",
    "            scores (Dict[str, dict]): CausalTune.scores dictionary\n",
    "            metric (str): metric of interest\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, dict]: dictionary containing best score by estimator\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        for k, v in scores.items():\n",
    "            if \"estimator_name\" not in v:\n",
    "                raise ValueError(\n",
    "                    f\"Malformed scores dict, 'estimator_name' field missing in {k}, {v}\"\n",
    "                )\n",
    "\n",
    "        estimator_names = sorted(\n",
    "            list(\n",
    "                set(\n",
    "                    [\n",
    "                        v[\"estimator_name\"]\n",
    "                        for v in scores.values()\n",
    "                        if \"estimator_name\" in v\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        best = {}\n",
    "        for name in estimator_names:\n",
    "            est_scores = [\n",
    "                v\n",
    "                for v in scores.values()\n",
    "                if \"estimator_name\" in v and v[\"estimator_name\"] == name\n",
    "            ]\n",
    "            best[name] = (\n",
    "                min(est_scores, key=lambda x: x[metric])\n",
    "                if metric in [\"energy_distance\", \"psw_energy_distance\"]\n",
    "                else max(est_scores, key=lambda x: x[metric])\n",
    "            )\n",
    "\n",
    "        return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Union\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "# implementation of https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957\n",
    "# we assume treatment takes integer values from 0 to n\n",
    "\n",
    "\n",
    "class DummyPropensity:\n",
    "    def __init__(self, p: pd.Series, treatment: pd.Series):\n",
    "        n_vals = max(treatment) + 1\n",
    "        out = np.zeros((len(treatment), n_vals))\n",
    "        for i, pp in enumerate(p.values):\n",
    "            out[i, treatment.values[i]] = pp\n",
    "        self.p = out\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self):\n",
    "        return self.p\n",
    "\n",
    "\n",
    "class ERUPT:\n",
    "    def __init__(\n",
    "        self,\n",
    "        treatment_name: str,\n",
    "        propensity_model,\n",
    "        X_names: Optional[List[str]] = None,\n",
    "        clip: float = 0.05,\n",
    "        remove_tiny: bool = True,\n",
    "    ):\n",
    "        self.treatment_name = treatment_name\n",
    "        self.propensity_model = copy.deepcopy(propensity_model)\n",
    "        self.X_names = X_names\n",
    "        self.clip = clip\n",
    "        self.remove_tiny = remove_tiny\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        if self.X_names is None:\n",
    "            self.X_names = [c for c in df.columns if c != self.treatment_name]\n",
    "        self.propensity_model.fit(df[self.X_names], df[self.treatment_name])\n",
    "\n",
    "    def score(\n",
    "        self, df: pd.DataFrame, outcome: pd.Series, policy: Callable\n",
    "    ) -> pd.Series:\n",
    "        w = self.weights(df, policy)\n",
    "        return (w * outcome).mean()\n",
    "\n",
    "    def weights(\n",
    "        self, df: pd.DataFrame, policy: Union[Callable, np.ndarray, pd.Series]\n",
    "    ) -> pd.Series:\n",
    "        W = df[self.treatment_name].astype(int)\n",
    "        assert all(\n",
    "            [x >= 0 for x in W.unique()]\n",
    "        ), \"Treatment values must be non-negative integers\"\n",
    "\n",
    "        if callable(policy):\n",
    "            policy = policy(df).astype(int)\n",
    "        if isinstance(policy, pd.Series):\n",
    "            policy = policy.values\n",
    "        policy = np.array(policy)\n",
    "\n",
    "        d = pd.Series(index=df.index, data=policy)\n",
    "        assert all(\n",
    "            [x >= 0 for x in d.unique()]\n",
    "        ), \"Policy values must be non-negative integers\"\n",
    "\n",
    "        if isinstance(self.propensity_model, DummyPropensity):\n",
    "            p = self.propensity_model.predict_proba()\n",
    "        else:\n",
    "            p = self.propensity_model.predict_proba(df[self.X_names])\n",
    "        p = np.maximum(p, 1e-4)\n",
    "\n",
    "        weight = np.zeros(len(df))\n",
    "\n",
    "        for i in W.unique():\n",
    "            weight[W == i] = 1 / p[:, i][W == i]\n",
    "\n",
    "        weight[d != W] = 0.0\n",
    "\n",
    "        if self.remove_tiny:\n",
    "            weight[weight > 1 / self.clip] = 0.0\n",
    "        else:\n",
    "            weight[weight > 1 / self.clip] = 1 / self.clip\n",
    "\n",
    "        weight *= len(df) / sum(weight)\n",
    "        assert not np.isnan(weight.sum()), \"NaNs in ERUPT weights\"\n",
    "\n",
    "        return pd.Series(index=df.index, data=weight)\n",
    "\n",
    "    def probabilistic_erupt_score(\n",
    "        self, df: pd.DataFrame, outcome: pd.Series, treatment_effects: pd.Series, treatment_std_devs: pd.Series, iterations: int = 1000\n",
    "    ) -> float:\n",
    "        unique_treatments = df[self.treatment_name].unique()\n",
    "        treatment_scores = {treatment: [] for treatment in unique_treatments}\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            sampled_effects = {\n",
    "                treatment: np.random.normal(treatment_effects.loc[treatment], treatment_std_devs.loc[treatment])\n",
    "                for treatment in unique_treatments\n",
    "            }\n",
    "            chosen_treatment = max(sampled_effects, key=sampled_effects.get)\n",
    "            # Compute weighted outcome\n",
    "            weights = self.weights(df, lambda x: np.array([chosen_treatment] * len(x)))\n",
    "            mean_outcome = (weights * outcome).sum() / weights.sum()\n",
    "            treatment_scores[chosen_treatment].append(mean_outcome)\n",
    "\n",
    "        average_outcomes = np.mean([np.mean(scores) for scores in treatment_scores.values() if scores])\n",
    "\n",
    "        return average_outcomes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
