{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # suppress sklearn deprecation warnings for now.. \n",
    "\n",
    "# the below checks for whether we run dowhy and auto-causality from source\n",
    "root_path = root_path = os.path.realpath('../..')\n",
    "try: \n",
    "    import auto_causality\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(os.path.join(root_path, \"auto-causality\"))\n",
    "    \n",
    "try:\n",
    "    import dowhy\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(os.path.join(root_path, \"dowhy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from auto_causality import AutoCausality\n",
    "from auto_causality.datasets import bdsianesi\n",
    "from auto_causality.data_utils import preprocess_dataset\n",
    "from auto_causality.scoring import ate, group_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = bdsianesi()\n",
    "treatment = 'ed'\n",
    "targets='wage'\n",
    "data_df, features_X, features_W = preprocess_dataset(data_df, treatment, targets)\n",
    "outcome = targets #[0]\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_vals = list(data_df[treatment].unique())\n",
    "treatment_vals.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you want to use specific estimators, comment in the estimator_list below\n",
    "# to include any estimators whose full name contains any of the elements of \n",
    "# estimator_list\n",
    "# The other allowed values are 'all' and 'auto', the default is 'auto'\n",
    "ac = AutoCausality(\n",
    "    time_budget=120, \n",
    "    estimator_list=[\n",
    "            \"Dummy\",\n",
    "            \"TransformedOutcome\",\n",
    "        ],\n",
    "    # metric=\"norm_erupt\", \n",
    "    verbose=1, # 3\n",
    "    components_verbose=1,# 2\n",
    "    components_time_budget=30,\n",
    "    treatment_val=treatment_vals\n",
    ")\n",
    "\n",
    "\n",
    "# run autocausality\n",
    "ac.fit(train_df, treatment, outcome, features_W, features_X)\n",
    "\n",
    "# return best estimator\n",
    "print(f\"Best estimator: {ac.best_estimator}\")\n",
    "# config of best estimator:\n",
    "print(f\"best config: {ac.best_config}\")\n",
    "# best score:\n",
    "print(f\"best score: {ac.best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.results.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score all estimators on the test set, which we've kept aside up till now\n",
    "from auto_causality.scoring import make_scores\n",
    "for est_name, scr in ac.scores.items():\n",
    "    scr['scores']['test'] = make_scores(scr['estimator'], test_df, ac.propensity_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "colors = ([matplotlib.colors.CSS4_COLORS['black']] +\n",
    "    list(matplotlib.colors.TABLEAU_COLORS) + [\n",
    "    matplotlib.colors.CSS4_COLORS['lime'],\n",
    "    matplotlib.colors.CSS4_COLORS['yellow'],\n",
    "    matplotlib.colors.CSS4_COLORS['pink']\n",
    "])\n",
    "\n",
    "\n",
    "plt.figure(figsize = (7,5))\n",
    "plt.title(outcome)\n",
    "\n",
    "m1 = \"erupt\"\n",
    "m2 = \"norm_erupt\"\n",
    "\n",
    "for (est, scr), col in zip(ac.scores.items(), colors):\n",
    "    try:\n",
    "        sc = [scr[\"scores\"]['train'][m1], scr[\"scores\"]['validation'][m1], scr[\"scores\"]['test'][m1]]\n",
    "        crv = [scr[\"scores\"]['train'][m2], scr[\"scores\"]['validation'][m2], scr[\"scores\"]['test'][m2]]\n",
    "        plt.plot(sc, crv, color=col, marker=\"o\", label=est.split(\".\")[-1])\n",
    "        plt.scatter(sc[1:2],crv[1:2], c=col, s=70, label=\"_nolegend_\")\n",
    "        plt.scatter(sc[2:],crv[2:], c=col, s=120, label=\"_nolegend_\")\n",
    "    except:\n",
    "        pass\n",
    "plt.xlabel(m1)\n",
    "plt.ylabel(m2)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.04,1), borderaxespad=0)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "scr = ac.scores[ac.best_estimator]\n",
    "intrp = scr[\"scores\"]['validation']['intrp']\n",
    "plt.figure(figsize=(15, 7))\n",
    "intrp.plot(feature_names=intrp.feature_names, fontsize=10)\n",
    "plt.title(f\"{ac.best_estimator}_{outcome}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add SHAP plots!\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# and now let's visualize feature importances!\n",
    "from auto_causality.shap import shap_values\n",
    "\n",
    "# Shapley values calculation can be slow so let's subsample\n",
    "this_df = test_df.sample(100)\n",
    "\n",
    "# ? Works only on specific models?\n",
    "scr = ac.scores[ac.best_estimator]\n",
    "print(outcome, ac.best_estimator)\n",
    "est = ac.model\n",
    "shaps = shap_values(est, this_df)\n",
    "\n",
    "plt.title(outcome + '_' + ac.best_estimator.split('.')[-1])\n",
    "shap.summary_plot(shaps, this_df[est.estimator._effect_modifier_names])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_causality.scoring import  group_ate\n",
    "# plot out-of sample difference of outcomes between treated and untreated for the points where a model predicts positive vs negative impact\n",
    "my_est = ac.best_estimator\n",
    "stats = []\n",
    "\n",
    "v = ac.scores[my_est]['scores']['test']['values']\n",
    "\n",
    "sts = group_ate(v['treated'], v[outcome], v['norm_policy'])\n",
    "\n",
    "display(sts)\n",
    "\n",
    "\n",
    "colors = (matplotlib.colors.CSS4_COLORS['black'],\n",
    "    matplotlib.colors.CSS4_COLORS['red'],\n",
    "    matplotlib.colors.CSS4_COLORS['blue'])\n",
    "\n",
    "grp = sts[\"policy\"].unique()\n",
    "\n",
    "for i,(p,c) in enumerate(zip(grp, colors)):\n",
    "    st = sts[sts[\"policy\"] == p]\n",
    "    plt.errorbar(np.array(range(len(st))) +0.1*i, st[\"mean\"].values[0],  yerr = st[\"std\"].values[0], color=c)\n",
    "plt.legend(grp)\n",
    "plt.grid(True)\n",
    "plt.title(my_est.split('.')[-1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('ac_dev': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "096974b08ddbbb985dcb097d79b571562f4c0f9bcb865e7057a2525491fdc62a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
